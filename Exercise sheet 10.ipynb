{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "bde62628-911f-4c38-8297-c60b795115f0",
      "metadata": {},
      "source": [
        "# Exercise sheet 10 - Parallelisation"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3f316e46-c426-461e-82e4-dc385cfca0f9",
      "metadata": {},
      "source": [
        "# Exercise 1 - Rigged dice\n",
        "\n",
        "Create a rigged dice function that 25% of the time returns the number 6. The rest of the time it returns the integers 1,2,3,4,5 uniformly.\n",
        "Test your function, by calling it **one billion times** (10^9) and checking that 6 is returned in the range of 249-251 million (inclusive) times. You do not need to check that numbers 1 to 5 are returned uniformly or randomly, but you need to check that your function returns integers in the range 1-6 (inclusive). **Time** how long it takes to run the script.\n",
        "\n",
        "Now attempt to **parallelise the task with a method of your own choosing** and time how long it takes once more. How does this compare to the previous *un-optimised* run?\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "id": "87afd3de",
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import multiprocessing\n",
        "import random\n",
        "import time\n",
        "import numpy as np\n",
        "import time\n",
        "import threading\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "id": "f97418cc",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "6\n",
            "Test passed! The number 6 appeared 25186 times.\n",
            "--- 0.7112221717834473 seconds 100k trials ---\n"
          ]
        }
      ],
      "source": [
        "start_time = time.time()\n",
        "\n",
        "def rigged_dice():\n",
        "    choices = [1, 2, 3, 4, 5, 6]\n",
        "    probabilities = [0.15, 0.15, 0.15, 0.15, 0.15, 0.25]\n",
        "    return np.random.choice(choices, p=probabilities)\n",
        "print(rigged_dice())\n",
        "\n",
        "#100k test\n",
        "trials = 100000\n",
        "test = [rigged_dice() for i in range(trials)]\n",
        "six_count = test.count(6)\n",
        "\n",
        "if 24700 <= six_count <= 25300:\n",
        "    print(f\"Test passed! The number 6 appeared {six_count} times.\")\n",
        "else:\n",
        "    print(f\"Test failed! The number 6 appeared {six_count} times.\")\n",
        "\n",
        "print(f\"--- {time.time() - start_time} seconds 100k trials ---\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "id": "8f488aba",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "number of processes: 12\n",
            "Total rolls: 1000000000\n",
            "Number of 6s rolled: 250000929\n",
            "Elapsed time: 47.69 seconds\n",
            "Number of 6s is within the expected range: 249000000 - 251000000\n"
          ]
        }
      ],
      "source": [
        "def rigged_dice():\n",
        "    if random.random() < 0.25:\n",
        "        return 6\n",
        "    else:\n",
        "        return random.randint(1, 5)\n",
        "\n",
        "def worker(num_iterations):\n",
        "    count_6 = 0\n",
        "    count_valid = 0\n",
        "    for _ in range(num_iterations):\n",
        "        result = rigged_dice()\n",
        "        if 1 <= result <= 6:\n",
        "            count_valid += 1\n",
        "            if result == 6:\n",
        "                count_6 += 1\n",
        "        else:\n",
        "            raise ValueError(f\"something went wrong: {result}\")\n",
        "    return (count_6, count_valid)\n",
        "\n",
        "def main():\n",
        "    TOTAL_ROLLS = 10**9\n",
        "    NUM_PROCESSES = multiprocessing.cpu_count()\n",
        "    CHUNK_SIZE = TOTAL_ROLLS // NUM_PROCESSES\n",
        "\n",
        "    print(f\"number of processes: {NUM_PROCESSES}\")\n",
        "\n",
        "    start_time = time.time()\n",
        "\n",
        "    with multiprocessing.Pool(processes=NUM_PROCESSES) as pool:\n",
        "        # Create a list of tasks\n",
        "        tasks = [CHUNK_SIZE] * NUM_PROCESSES\n",
        "        # Handle any remaining rolls\n",
        "        remaining = TOTAL_ROLLS % NUM_PROCESSES\n",
        "        if remaining:\n",
        "            tasks.append(remaining)\n",
        "        \n",
        "        # Map the worker function to the pool\n",
        "        results = pool.map(worker, tasks)\n",
        "\n",
        "    total_6s = sum(result[0] for result in results)\n",
        "    #total_valid = sum(result[1] for result in results)\n",
        "\n",
        "    end_time = time.time()\n",
        "    elapsed_time = end_time - start_time\n",
        "\n",
        "    print(f\"Total rolls: {TOTAL_ROLLS}\")\n",
        "    print(f\"Number of 6s rolled: {total_6s}\")\n",
        "    print(f\"Elapsed time: {elapsed_time:.2f} seconds\")\n",
        "    # Check if 6s are within the expected range\n",
        "    expected_min = 249_000_000\n",
        "    expected_max = 251_000_000\n",
        "    if expected_min <= total_6s <= expected_max:\n",
        "        print(f\"Number of 6s is within the expected range: {expected_min} - {expected_max}\")\n",
        "    else:\n",
        "        print(f\"Number of 6s is outside the expected range: {total_6s}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5547c0f3",
      "metadata": {},
      "source": [
        "# Exercise 2 - Calculate $\\pi$\n",
        "\n",
        "Using the **DSMC method**, calculate the value of **$\\pi$**.\n",
        "\n",
        "\n",
        "**Approach:**\n",
        "In order to do this, create a 2-dimensional domain (defined by the coordinates $x_{min}, x_{max}, y_{min}, y_{max}$) and launch a number P of particles at random locations within. Check which particles lie inside a circle with radius $$ \\frac{x_{max}-x_{min}}{2}, $$ where $x_{min}, x_{max}$ are the x-limits of your 2D domain. \n",
        "\n",
        "Get your value for $\\pi$ by using the following formula:\n",
        "$\\pi = \\frac{4 \\cdot n_{inside}}{P},$ where $n_{inside}$ is the number of particles inside the circle and $P$ is the total number of particles.\n",
        "\n",
        "Play around with the number of particles. \n",
        "\n",
        "a) Try to improve this task by making use of threading (you can use either the **_thread** or **threading** module). What are your findings, is the script running faster?\n",
        "\n",
        "b) Now try to improve the running time of the code by employing the **multiprocessing** module. Are there any differences as compared to threading?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "id": "69cb744b",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Particles: 100000000\n",
            "Estimated π: 3.14135396\n",
            "Time elapsed: 1.75 seconds\n"
          ]
        }
      ],
      "source": [
        "def estimate_pi(total_particles=10_000_000_0, x_min=0, x_max=1, y_min=0, y_max=1):\n",
        "    radius = (x_max - x_min) / 2\n",
        "    center = x_min + radius\n",
        "\n",
        "    start_time = time.time()\n",
        "\n",
        "    x = np.random.uniform(x_min, x_max, total_particles)\n",
        "    y = np.random.uniform(y_min, y_max, total_particles)\n",
        "    distances_squared = (x - center)**2 + (y - center)**2\n",
        "    n_inside = np.sum(distances_squared <= radius**2)\n",
        "    pi_estimate = (4 * n_inside) / total_particles\n",
        "\n",
        "    elapsed_time = time.time() - start_time\n",
        "\n",
        "    return total_particles, pi_estimate, elapsed_time\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    particles, pi, elapsed = estimate_pi()\n",
        "    print(f\"Particles: {particles}\")\n",
        "    print(f\"Estimated π: {pi}\")\n",
        "    print(f\"Time elapsed: {elapsed:.2f} seconds\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d2d3b1ab",
      "metadata": {},
      "source": [
        "### (A)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "aa1d9f6b",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Particles: 100000000\n",
            "Estimated π: 3.14167308\n",
            "Time elapsed: 1.17 seconds\n"
          ]
        }
      ],
      "source": [
        "def estimate_pi_threaded(total_particles=10_000_000_0, num_threads=6, x_min=0, x_max=1, y_min=0, y_max=1):\n",
        "    radius = (x_max - x_min) / 2\n",
        "    center = x_min + radius\n",
        "\n",
        "    particles_per_thread = total_particles // num_threads\n",
        "    remaining_particles = total_particles % num_threads\n",
        "\n",
        "    counts = []\n",
        "    lock = threading.Lock()\n",
        "\n",
        "    def worker(n_particles):\n",
        "        nonlocal counts\n",
        "        x = np.random.uniform(x_min, x_max, n_particles)\n",
        "        y = np.random.uniform(y_min, y_max, n_particles)\n",
        "        distances_squared = (x - center)**2 + (y - center)**2\n",
        "        n_inside = np.sum(distances_squared <= radius**2)\n",
        "        with lock:\n",
        "            counts.append(n_inside)\n",
        "\n",
        "    threads = []\n",
        "    start_time = time.time()\n",
        "\n",
        "    #  threads\n",
        "    for i in range(num_threads):\n",
        "        # Distribute remaining particles to the last thread\n",
        "        if i == num_threads - 1:\n",
        "            n_particles = particles_per_thread + remaining_particles\n",
        "        else:\n",
        "            n_particles = particles_per_thread\n",
        "        thread = threading.Thread(target=worker, args=(n_particles,))\n",
        "        threads.append(thread)\n",
        "        thread.start()\n",
        "\n",
        "    # Wait for all threads to finish\n",
        "    for thread in threads:\n",
        "        thread.join()\n",
        "\n",
        "\n",
        "    total_inside = sum(counts)\n",
        "    pi_estimate = (4 * total_inside) / total_particles\n",
        "    elapsed_time = time.time() - start_time\n",
        "\n",
        "    return total_particles, pi_estimate, elapsed_time\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    PARTICLES, PI, TIME_ELAPSED = estimate_pi_threaded()\n",
        "    print(f\"Particles: {PARTICLES}\")\n",
        "    print(f\"Estimated π: {PI}\")\n",
        "    print(f\"Time elapsed: {TIME_ELAPSED:.2f} seconds\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cfbb98a6",
      "metadata": {},
      "source": [
        "### (B)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9be6dcfc",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Particles: 100000000\n",
            "Estimated π: 3.14120812\n",
            "Time elapsed: 0.73 seconds\n"
          ]
        }
      ],
      "source": [
        "def count_inside_circle(n_particles, x_min, x_max, y_min, y_max):\n",
        "    radius = (x_max - x_min) / 2\n",
        "    center = x_min + radius\n",
        "\n",
        "    x = np.random.uniform(x_min, x_max, n_particles)\n",
        "    y = np.random.uniform(y_min, y_max, n_particles)\n",
        "    distances_squared = (x - center)**2 + (y - center)**2\n",
        "    return np.sum(distances_squared <= radius**2)\n",
        "\n",
        "def estimate_pi_multiprocessing(total_particles=10_000_000_0, num_processes=None, x_min=0, x_max=1, y_min=0, y_max=1):\n",
        "    if num_processes is None:\n",
        "        num_processes = multiprocessing.cpu_count()\n",
        "\n",
        "    particles_per_process = total_particles // num_processes\n",
        "    remaining_particles = total_particles % num_processes\n",
        "\n",
        "    # list with the number of particles for each process\n",
        "    tasks = [particles_per_process] * num_processes\n",
        "    if remaining_particles:\n",
        "        tasks[-1] += remaining_particles  # Add the remainder to the last process\n",
        "\n",
        "    start_time = time.time()\n",
        "\n",
        "    with multiprocessing.Pool(processes=num_processes) as pool:\n",
        "        results = pool.starmap(\n",
        "            count_inside_circle,\n",
        "            [(n, x_min, x_max, y_min, y_max) for n in tasks]\n",
        "        )\n",
        "\n",
        "    total_inside = sum(results)\n",
        "    pi_estimate = (4 * total_inside) / total_particles\n",
        "    elapsed_time = time.time() - start_time\n",
        "\n",
        "    return total_particles, pi_estimate, elapsed_time\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    PARTICLES, PI, TIME_ELAPSED = estimate_pi_multiprocessing()\n",
        "    print(f\"Particles: {PARTICLES}\")\n",
        "    print(f\"Estimated π: {PI}\")\n",
        "    print(f\"Time elapsed: {TIME_ELAPSED:.2f} seconds\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "abe04167-4133-46d8-bd93-796b45f0fd1f",
      "metadata": {},
      "source": [
        "# Exercise 3 - Why do we parallelise?\n",
        "\n",
        "Make a one slide presentation of some of the things you consider important when it comes to parallelising code (in Python) or why it's not important, based on the things discussed during lecture, found in documentation, online forums, experimenting with your code etc.\n",
        "\n",
        "Keep it short and to the point, no minimum time requirement. You can do it as a bunch of comments as part of your code or as a simple ASCII file, anything goes as long as the points are presented in a coherent manner."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "12cc7640",
      "metadata": {},
      "source": [
        "## Parallelizing Code in Python\n",
        "\n",
        "### **1. Task Nature**\n",
        "- **CPU-bound:** \n",
        "  - **Use:** `multiprocessing` to leverage multiple cores.\n",
        "  - **Reason:** Bypasses the Global Interpreter Lock (GIL).\n",
        "- **I/O-bound:** \n",
        "  - **Use:** `threading` or `asyncio` for concurrent operations.\n",
        "  - **Reason:** Efficiently handles waiting times without heavy CPU usage.\n",
        "\n",
        "### **2. Global Interpreter Lock (GIL)**\n",
        "- **Impact:** \n",
        "  - Restricts execution of multiple native threads in CPU-bound tasks.\n",
        "- **Solution:** \n",
        "  - **Multiprocessing:** Creates separate processes, each with its own Python interpreter.\n",
        "\n",
        "### **3. Overhead and Complexity**\n",
        "- **Process/Thread Management:** \n",
        "  - **Overhead:** Increased memory and time costs for creating and managing threads/processes.\n",
        "  - **Best Practice:** Parallelize only when tasks are substantial enough to offset overhead.\n",
        "- **Debugging Challenges:** \n",
        "  - More complex to debug due to potential race conditions and deadlocks.\n",
        "\n",
        "### **4. Data Sharing and Synchronization**\n",
        "- **Multiprocessing:** \n",
        "  - **Memory:** Separate memory spaces.\n",
        "  - **Communication:** Use inter-process communication (IPC) mechanisms like queues or pipes.\n",
        "- **Threading:** \n",
        "  - **Memory:** Shared memory space.\n",
        "  - **Synchronization:** Requires locks or other synchronization primitives to prevent data corruption.\n",
        "\n",
        "### **5. Scalability**\n",
        "- **Multiprocessing:** \n",
        "  - Scales well with the number of CPU cores.\n",
        "- **Threading:** \n",
        "  - More suited for handling multiple I/O tasks rather than CPU-intensive operations.\n",
        "\n",
        "### **6. Libraries and Tools**\n",
        "- **High-Level APIs:** \n",
        "  - `concurrent.futures` offers `ThreadPoolExecutor` and `ProcessPoolExecutor` for simplified parallelism.\n",
        "- **NumPy and Other Libraries:** \n",
        "  - Some operations release the GIL, allowing true parallelism even with threading.\n",
        "\n",
        "---\n",
        "\n",
        "## **When to Parallelize**\n",
        "- **Independent Tasks:** Tasks that don't rely on each other's results.\n",
        "- **Performance Gains:** When the computational benefits outweigh the overhead.\n",
        "- **Scalable Workloads:** Suitable for applications that can scale with more cores.\n",
        "\n",
        "## **When Not to Parallelize**\n",
        "- **Simple or Quick Tasks:** Overhead may negate any performance improvements.\n",
        "- **Tightly Coupled Tasks:** High interdependency can lead to complex synchronization issues.\n",
        "- **Memory Constraints:** Limited memory may not support multiple processes effectively.\n",
        "\n",
        "---\n",
        "\n",
        "## **Conclusion**\n",
        "Parallelizing code in Python can lead to significant performance improvements, especially for CPU-bound and large-scale tasks. However, it's essential to assess the nature of the task, understand the implications of the GIL, and weigh the added complexity against the potential benefits. Utilizing the right tools and libraries can simplify the process and help achieve efficient parallelism.\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a4ea3e7c",
      "metadata": {},
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
